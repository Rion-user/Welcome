{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdBchyK079X+MqPo2LE4vd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rion-user/Welcome/blob/master/%EC%9E%90%EB%A3%8C%EC%A1%B0%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자료조사"
      ],
      "metadata": {
        "id": "mYc-AgVrsJSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "링크\n",
        "\n",
        "[2023 인공지능 학습용 데이터 품질관리 가이드라인 및 구축 안내서 v3.0](https://www.nia.or.kr/site/nia_kor/ex/bbs/View.do?cbIdx=26537&bcIdx=25370&parentSeq=25370)\n",
        "\n",
        "[An Introduction to Machine Translation Quality Estimation](https://phrase.com/blog/posts/mt-quality-estimation/)\n",
        "\n",
        "\n",
        "\n",
        "keywords\n",
        "* TER(번역 편집 거리)\n",
        "* 기계번역 품질 예측 태스크\n",
        "* 기계번역 품질 예측 모델의 평가는 Matthews correlation coefficient (MCC)로 측정하며, f1 score도 함께 평가 지표로 제시됨\n"
      ],
      "metadata": {
        "id": "D95r6YE3sK_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 파파고 빨간펜\n",
        "* mqm 지표"
      ],
      "metadata": {
        "id": "g23KqukssiLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파파고 빨간펜 선생님\n",
        "> 번역모델 평가에는 크게 2가지 방식이 존재\n",
        "1. 전문가 평가 : 기계번역 모델간 품질을 가장 정확히 비교 및 평가하는 방법\n",
        "  - 시간 및 비용 측면에서 비쌈\n",
        "  - 평가 데이터 구축 필요함\n",
        "  - 현 ML 산업은 매우 fast-pace, 서비스 모델 개선/업데이트 인터벌이 짧음\n",
        "2. 자동 번역 평가\n",
        "  - 전문가 평가 대비 적은 비용과 시간 투입\n",
        "  - BLEU를 일반적으로 많이 쓰지만 정밀한(semantic) 평가 불가\n",
        "\n",
        "WMT 2020~2022 QE Shared Task 참여하며 자체 기술 고도화\n",
        "> QE Task는 목적에 따라 QE 모델의 예측값 형태도 다양함\n",
        "\n",
        "파파고는 똑똑해 <-> Papago is cute라는 세트가 존재할 때\n",
        "* Sentence-level QE : 0.3\n",
        "* Word-level QE : papago = Good, is = Good, cute = Bad\n",
        "* MQM word-level QE : cute = error_type : mistranslation, error_severity : major\n",
        "\n",
        "## 파파고 QE 모델 기술\n",
        "\n",
        "### 1. 인공 학습데이터 생성을 통해 데이터 양 / 언어쌍을 보강\n",
        "* 공개된 QE 모델을 사용하여 (원문, 번역문) 문장쌍에 대한 인공 레이블을 생성\n",
        "* 인공데이터를 학습에 추가 사용시, 모든 언어쌍에서 성능 향상\n",
        "\n",
        "### 2. Parallel Mining\n",
        "* Monolingual corpus를 NMT에 활용하는 방법\n",
        "* 모델 기반 : BART, MASS pretrain -> NMT fine-tune\n",
        "* 데이터 증강 기반 : Back-translation\n",
        "* 데이터 증강 기반 : Parallel Mining\n",
        "  * 대량의 한국어, 영어 말뭉치가 존재할 때 한-영 alignment를 탐색"
      ],
      "metadata": {
        "id": "FEEXHgvztoGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WMT QE Shared TASK 2023\n",
        "\n",
        "> Goals\n",
        "\n",
        "In addition to generally advancing the state of the art in quality estimation, our specific goals are:\n",
        "\n",
        "* to extend the available public benchmark datasets with medium- and low-resource languages;\n",
        "* to investigate the potential of fine-graned quality estimation;\n",
        "to investigate new multilingual and language independent approaches esp. for zero-shot approaches; and\n",
        "* to study the robustness of QE approaches\n",
        "\n",
        "Specifically based on the provided MQM annotations we compute the MQM error by summing penalties for each error category:\n",
        "\n",
        "* +1 point for minor errors\n",
        "* +5 points for major errors\n",
        "* +10 points for critical errors\n",
        "\n",
        "To align with DA annotations we subtract the summed penalties from **100 (perfect score)** and we then divide by the sentence length (computed as number of words). We finally standardize the scores for each language pair/annotator.\n",
        "\n",
        "> Evaluation\n",
        "\n",
        "Word-level : We will use **MCC** (Matthews correlation coefficient) as a primary metric and **F1-score** as secondary.\n",
        "\n",
        "> Submission Format\n",
        "\n",
        "(완벽하게 매칭하지는 않음 참고사항)\n",
        "* **LANGUAGE PAIR** is the ID (e.g. en-de) of the language pair of the plain text translation file you are scoring. Follow the LP naming convention provided in the test set.\n",
        "* **METHOD NAME** is the name of your quality estimation method.\n",
        "* **SEGMENT NUMBER** is the line number of the plain text translation file you are scoring (starting at 0).\n",
        "* **TARGET SENTENCE** is the target sentence based on which the error span indices were extracted. You should use exactly the target sentence as provided by the test set to ensure alignment with the gold labels.\n",
        "* **ERROR START INDICES** the start indices (character level) of every exrror span extracted. For multiple error spans separate indices by a whitespace. For no errors output -1.\n",
        "* **ERROR END INDICES** the end indices (character level) of every exrror span extracted. For multiple error spans separate indices by a whitespace. For no errors output -1.\n",
        "* **ERROR TYPES** indication of minor or major error for each detected error span. The number of indices should match the number of errors. If there is no error span in a segment indicate with no-error.\n",
        "\n",
        "> Description of Error\n",
        "\n",
        "For a description of error severtities and MQM annotations you can also read:\n",
        "\n",
        "* Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation\n",
        "* Results of the WMT21 Metrics Shared Task\n",
        "\n",
        "> Useful Software\n",
        "\n",
        "Here are some open source software for QE that might be useful for participants: (모델 학습 Framework라 도움은 안될 듯)\n",
        "\n",
        "* OpenKiwi\n",
        "* COMET-QE\n",
        "* TransQuest\n",
        "* DeepQuest"
      ],
      "metadata": {
        "id": "ah9hlkpZ5SOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation\n",
        "\n",
        "[Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation](https://aclanthology.org/2021.tacl-1.87) (Freitag et al., TACL 2021)\n",
        "\n",
        "[유튜브 보기](https://www.youtube.com/watch?reload=9&app=desktop&v=EBTaGnggVR0)\n",
        "\n",
        "### Abstract\n",
        "\n",
        "field still lacks a commonlyaccepted standard procedure\n",
        "\n",
        "toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework.\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Like many natural language generation tasks, machine translation (MT) is difficult to evaluate because the set of correct answers for each input is\n",
        "large and usually unknown.\n",
        "\n",
        "Yet even human evaluation is problematic\n",
        "\n",
        "there is a risk that the signal will become lost in rater noise or bias.\n",
        "\n",
        "This paper aims to contribute to the evolution of\n",
        "standard practices for human evaluation of highquality MT.\n",
        "\n",
        "Our key insight is that any scoring or ranking of translations is implicitly based on an identification of errors and other imperfections.\n",
        "\n",
        "MQM is a generic framework that provides a\n",
        "hierarchy of translation errors which can be tailored to specific applications. We identified a hierarchy appropriate for broad-coverage MT, and\n",
        "annotated outputs from 10 top-performing \"systems\" (including human references)\n",
        "\n",
        "Our main contributions are:\n",
        "\n",
        "* A proposal for a standard MQM scoring\n",
        "scheme appropriate for broad-coverage MT.\n",
        "* Demonstration that automatic metrics based on pre-trained embeddings can outperform human crowd workers.\n",
        "* Characterization of current error types in HT and MT, identifying specific MT weaknesses\n",
        "\n",
        "### MQM\n",
        "Table 12. MQM Guidelines\n",
        "\n",
        "Since we are ultimately interested in scoring\n",
        "segments, we require a weighting on error types.\n",
        "We fixed the weight on Minor errors at 1, and explored a range of Major weights from 1 to 10 (the\n",
        "Major weight recommended in the MQM standard). For each weight combination we examined\n",
        "the stability of system ranking using a resampling\n",
        "technique. We found that a Major weight of 5 gave\n",
        "the best balance between stability and ability to\n",
        "discriminate among systems.\n",
        "\n",
        "These weights apply to all error categories with\n",
        "two exceptions. We assigned a weight of 0.1 to\n",
        "Minor Fluency/Punctuation errors to reflect their\n",
        "mostly non-linguistic nature. Decisions like the\n",
        "style of quotation mark to use or the spacing\n",
        "around punctuation affect the appearance of a text\n",
        "but do not change its meaning. Unlike other kinds\n",
        "of Minor errors, these are easy to correct algorithmically, so we assign a low weight to ensure that\n",
        "their main role is to distinguish between systems\n",
        "that are equivalent in other respects. Major Fluency/Punctuation errors, which render a text ungrammatical or change its meaning (eg, eliding\n",
        "the comma in “Let’s eat, grandma”), have standard weighting. The second exception is the singleton Non-translation category, with a weight of\n",
        "25, equivalent to five Major errors.\n",
        "Table 1 summarizes our weighting scheme, in\n",
        "which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score\n",
        "is an average over scores from all annotators.\n",
        "\n",
        "| a | b | c |\n",
        "|---|---|---|\n",
        "| Major  | Non-translation  | 25  |\n",
        "| Major  | all others  |  5 |\n",
        "| Minor  | Fluency/Punctuation  | 0.1  |\n",
        "| Minor  | all others | 1  |\n",
        "| Netural  | all  | 0  |\n",
        "\n",
        "Table 1: MQM error weighting\n",
        "\n",
        "실험은 유튜브 참조\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "As part of this work, we proposed a standard\n",
        "MQM scoring scheme that is appropriate for high quality MT.\n",
        "\n",
        "Our study shows that crowd-worker human\n",
        "evaluations (as conducted by WMT) have low\n",
        "correlation with MQM, and the resulting system level rankings are quite different.\n",
        "\n",
        "MQM labels acquired with professional translators show a large\n",
        "gap between the quality of human and machine\n",
        "generated translations. This demonstrates that\n",
        "MT is still far from human parity. Furthermore,\n",
        "we characterize the current error types in human\n",
        "and machine translations, highlighting which error\n",
        "types are responsible for the difference between\n",
        "the two.\n",
        "\n",
        " We hope that researchers will use this\n",
        "as motivation to establish more error-type specific\n",
        "research directions.\n",
        "\n",
        "### 추가\n",
        "Many MQM schemes include an\n",
        "additional “Critical” severity which is worse than\n",
        "Major, but we dropped this because its definition\n",
        "is often context-specific, capturing errors that are\n",
        "disproportionately harmful for a particular application\n",
        "\n",
        "We felt that for broad coverage MT the\n",
        "distinction between Major and Critical was likely\n",
        "to be highly subjective, while Major errors (actual\n",
        "errors) would be easier to distinguish from Minor\n",
        "ones (imperfections). Neutral severity allows annotators to express subjective opinions about the\n",
        "translation without affecting its rating\n",
        "\n",
        "Annotator instructions are shown in Table 12.\n",
        "We kept these minimal because our raters were\n",
        "professionals with previous experience in assessing translation quality, including with MQM.\n",
        "There are many subtle issues that arise in error annotation, such as the correct way to translate units\n",
        "(eg, should 1 inch be translated as 1 Zoll, 1cm,\n",
        "or 2.54cm?), but we resisted the temptation to establish an extensive list of context-specific guidelines, relying instead on the judgment of our annotators. In order to temper the effect of long segments, we imposed a maximum of five errors per\n",
        "segment. For segments with more errors, we asked\n",
        "raters to identify only the five most severe. Thus\n",
        "we do not distinguish between segments containing five or more than five Major errors, although\n",
        "we do distinguish between segments with many\n",
        "identifiable errors and those that are categorized\n",
        "as entirely Non-translation. To focus our raters\n",
        "on careful error identification, and to provide potentially useful information for further studies, we\n",
        "had them highlight error spans in the text, following the conventions laid out in Table 12.\n",
        "\n",
        "Scoring\n",
        "Since we are ultimately interested in deriving\n",
        "scores for sentences, we require a weighting on\n",
        "error categories and severities. We set the weight\n",
        "on Minor errors to 1, and explored a range of Major error weights from 1 to 10 (the Major weight\n",
        "recommended in the MQM standard). For each\n",
        "weight combination we examined the stability of\n",
        "system ranking using a resampling technique. We\n",
        "found that a Major weight of 5 gave the best balance of stability and ability to discriminate among\n",
        "systems.\n",
        "These weights apply to all error categories except Fluency/Punctuation and Nontranslation. We assigned a weight of 0.1 for\n",
        "Fluency/Punctuation to reflect its mostly nonlinguistic character. Decisions like the kind of\n",
        "quotation mark to use or the spacing between\n",
        "words and punctuation affect the appearance of a\n",
        "text but do not change its meaning. Unlike other\n",
        "kinds of minor errors, these are easy to correct\n",
        "algorithmically, so we assign them a low weight\n",
        "to ensure that their main role is to distinguish\n",
        "between systems that are equivalent in other\n",
        "respects. Our decision is supported by evidence\n",
        "from professional translators, who tend to treat\n",
        "minor punctuation errors as insignificant for the\n",
        "purpose of scoring, even when they are required to\n",
        "annotate them within the MQM framework. Note\n",
        "that this category does not include punctuation\n",
        "errors that render a text ungrammatical or change\n",
        "its meaning (eg, eliding the comma in “Let’s eat,\n",
        "grandma”), which have the same weight as other\n",
        "Major errors. Source errors are ignored in our\n",
        "current study but give us the ability to discard\n",
        "badly garbled source sentences, which might be\n",
        "prevalent in certain genres. The singleton Nontranslation category has a weight of 25, equivalent\n",
        "to five Major errors, the worst segment-level score\n",
        "possible in our annotation scheme.\n",
        "Our current weighting ignores the text span of\n",
        "errors, as this provides little information relevant\n",
        "to scoring once severity and category are taken\n",
        "into account.\n",
        "Table 1 summarizes our weighting scheme. The\n",
        "score of a segment is the sum of all errors it contains, averaged over all annotators, and ranges\n",
        "from 0 (perfect) to 25 (maximally bad). Segment scores are averaged to provide document and system-level scores.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0B8QoV6pOfWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MQM Framework"
      ],
      "metadata": {
        "id": "0GY354J4QuvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expert-based Human Evaluations on TED and News Domain\n",
        "\n",
        "[Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expert-based Human Evaluations on TED and News Domain](https://aclanthology.org/2021.wmt-1.73) (Freitag et al., WMT 2021)\n",
        "\n",
        "기존 WMT의 경우 new data 기준 해당 저자는 metrics의 generalize\n",
        "and perform well across domains를 판단하기 위해 Ted talks 데이터를 추가\n",
        "\n",
        "### Ted Talks Test Suite\n",
        "\n",
        "A long standing question about automated MT evaluation metrics has been whether metrics generalize and perform well across domains\n",
        "\n",
        "The TED domain is quite different from the news domain, particularly in its more informal and disfluent language style, yet it covers a wide variety\n",
        "of topics and vocabularies.\n",
        "\n",
        "As we will see, the quality performance\n",
        "of these systems and their relative rankings can be\n",
        "quite different depending on the language pair, as\n",
        "these were not trained to yield the highest performance on the news or TED domain\n",
        "\n",
        "The quality of the underlying human ratings is critical and recent findings (Freitag et al., 2021) have shown that crowd-sourced human ratings are not reliable for high quality MT output.\n",
        "\n",
        "To temper the effect of long segments, we imposed a maximum of five errors per segment, instructing raters to choose the five most severe errors for segments containing more errors.\n",
        "\n",
        "->  구글이랑 같은거 썼다. weighting도 같음.\n",
        "\n",
        "Annotation for English→Russian was performed\n",
        "by Unbabel who used a single professional native\n",
        "language annotator with several years of translation\n",
        "error experience based on variations of the MQM\n",
        "framework (Lommel et al., 2014).\n",
        "\n",
        "The Unbabel\n",
        "severity options differ slightly from that of Google\n",
        "in that we also specify a ‘critical’ error severity and\n",
        "do not specify a ‘neutral’ category.\n",
        "\n",
        "-> 근데 구글이랑 좀 다르게 함.\n",
        "\n",
        "Additionally,\n",
        "in the Unbabel typology, all error categories are\n",
        "weighted equally within each severity level.\n",
        "\n",
        "MQM scores at a segment level are calculated\n",
        "by summing the number of errors in the segment\n",
        "in each severity and applying a severity weight as\n",
        "described in Table 3. In contrast to the Google\n",
        "scheme, Unbabel does not impose a limit on the\n",
        "number of errors in a segment.\n",
        "\n",
        "-> 계산 방식도 좀 다름 (Table 3 참고)\n",
        "\n",
        "### Google vs. Unbabel MQM\n",
        "\n",
        "Given that annotations were undertaken for\n",
        "English→Russian using a different setup and\n",
        "MQM scheme than those for English→German\n",
        "and English→Chinese we sought to provide some\n",
        "insight into the compatibility of the two schemes\n",
        "by repeating the annotation for English→German\n",
        "using Unbabel’s scheme and annotator pool\n",
        "\n",
        "We note that the Google annotators left 59.5% of\n",
        "the sample untouched (i.e. error free), whereas the\n",
        "Unbabel annotator left only 46.9% untouched. It\n",
        "appears that the Unbabel annotator was on average\n",
        "more aggressive in their annotation which might\n",
        "partially explain low levels of agreement.\n",
        "\n"
      ],
      "metadata": {
        "id": "tCKS28ok7RKr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps3Iz4QysHY4"
      },
      "outputs": [],
      "source": []
    }
  ]
}