{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXpTR3QG0Eva99j5FUk/iy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rion-user/Welcome/blob/master/%EC%9E%90%EB%A3%8C%EC%A1%B0%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자료조사"
      ],
      "metadata": {
        "id": "mYc-AgVrsJSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "링크\n",
        "\n",
        "[2023 인공지능 학습용 데이터 품질관리 가이드라인 및 구축 안내서 v3.0](https://www.nia.or.kr/site/nia_kor/ex/bbs/View.do?cbIdx=26537&bcIdx=25370&parentSeq=25370)\n",
        "\n",
        "[An Introduction to Machine Translation Quality Estimation](https://phrase.com/blog/posts/mt-quality-estimation/)\n",
        "\n",
        "\n",
        "\n",
        "keywords\n",
        "* TER(번역 편집 거리)\n",
        "* 기계번역 품질 예측 태스크\n",
        "* 기계번역 품질 예측 모델의 평가는 Matthews correlation coefficient (MCC)로 측정하며, f1 score도 함께 평가 지표로 제시됨\n"
      ],
      "metadata": {
        "id": "D95r6YE3sK_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 파파고 빨간펜\n",
        "* mqm 지표"
      ],
      "metadata": {
        "id": "g23KqukssiLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파파고 빨간펜 선생님\n",
        "> 번역모델 평가에는 크게 2가지 방식이 존재\n",
        "1. 전문가 평가 : 기계번역 모델간 품질을 가장 정확히 비교 및 평가하는 방법\n",
        "  - 시간 및 비용 측면에서 비쌈\n",
        "  - 평가 데이터 구축 필요함\n",
        "  - 현 ML 산업은 매우 fast-pace, 서비스 모델 개선/업데이트 인터벌이 짧음\n",
        "2. 자동 번역 평가\n",
        "  - 전문가 평가 대비 적은 비용과 시간 투입\n",
        "  - BLEU를 일반적으로 많이 쓰지만 정밀한(semantic) 평가 불가\n",
        "\n",
        "WMT 2020~2022 QE Shared Task 참여하며 자체 기술 고도화\n",
        "> QE Task는 목적에 따라 QE 모델의 예측값 형태도 다양함\n",
        "\n",
        "파파고는 똑똑해 <-> Papago is cute라는 세트가 존재할 때\n",
        "* Sentence-level QE : 0.3\n",
        "* Word-level QE : papago = Good, is = Good, cute = Bad\n",
        "* MQM word-level QE : cute = error_type : mistranslation, error_severity : major\n",
        "\n",
        "## 파파고 QE 모델 기술\n",
        "\n",
        "### 1. 인공 학습데이터 생성을 통해 데이터 양 / 언어쌍을 보강\n",
        "* 공개된 QE 모델을 사용하여 (원문, 번역문) 문장쌍에 대한 인공 레이블을 생성\n",
        "* 인공데이터를 학습에 추가 사용시, 모든 언어쌍에서 성능 향상\n",
        "\n",
        "### 2. Parallel Mining\n",
        "* Monolingual corpus를 NMT에 활용하는 방법\n",
        "* 모델 기반 : BART, MASS pretrain -> NMT fine-tune\n",
        "* 데이터 증강 기반 : Back-translation\n",
        "* 데이터 증강 기반 : Parallel Mining\n",
        "  * 대량의 한국어, 영어 말뭉치가 존재할 때 한-영 alignment를 탐색"
      ],
      "metadata": {
        "id": "FEEXHgvztoGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WMT QE Shared TASK 2023\n",
        "\n",
        "> Goals\n",
        "\n",
        "In addition to generally advancing the state of the art in quality estimation, our specific goals are:\n",
        "\n",
        "* to extend the available public benchmark datasets with medium- and low-resource languages;\n",
        "* to investigate the potential of fine-graned quality estimation;\n",
        "to investigate new multilingual and language independent approaches esp. for zero-shot approaches; and\n",
        "* to study the robustness of QE approaches\n",
        "\n",
        "Specifically based on the provided MQM annotations we compute the MQM error by summing penalties for each error category:\n",
        "\n",
        "* +1 point for minor errors\n",
        "* +5 points for major errors\n",
        "* +10 points for critical errors\n",
        "\n",
        "To align with DA annotations we subtract the summed penalties from **100 (perfect score)** and we then divide by the sentence length (computed as number of words). We finally standardize the scores for each language pair/annotator.\n",
        "\n",
        "> Evaluation\n",
        "\n",
        "Word-level : We will use **MCC** (Matthews correlation coefficient) as a primary metric and **F1-score** as secondary.\n",
        "\n",
        "> Submission Format\n",
        "\n",
        "(완벽하게 매칭하지는 않음 참고사항)\n",
        "* **LANGUAGE PAIR** is the ID (e.g. en-de) of the language pair of the plain text translation file you are scoring. Follow the LP naming convention provided in the test set.\n",
        "* **METHOD NAME** is the name of your quality estimation method.\n",
        "* **SEGMENT NUMBER** is the line number of the plain text translation file you are scoring (starting at 0).\n",
        "* **TARGET SENTENCE** is the target sentence based on which the error span indices were extracted. You should use exactly the target sentence as provided by the test set to ensure alignment with the gold labels.\n",
        "* **ERROR START INDICES** the start indices (character level) of every exrror span extracted. For multiple error spans separate indices by a whitespace. For no errors output -1.\n",
        "* **ERROR END INDICES** the end indices (character level) of every exrror span extracted. For multiple error spans separate indices by a whitespace. For no errors output -1.\n",
        "* **ERROR TYPES** indication of minor or major error for each detected error span. The number of indices should match the number of errors. If there is no error span in a segment indicate with no-error.\n",
        "\n",
        "> Description of Error\n",
        "\n",
        "For a description of error severtities and MQM annotations you can also read:\n",
        "\n",
        "* Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation\n",
        "* Results of the WMT21 Metrics Shared Task\n",
        "\n",
        "> Useful Software\n",
        "\n",
        "Here are some open source software for QE that might be useful for participants: (모델 학습 Framework라 도움은 안될 듯)\n",
        "\n",
        "* OpenKiwi\n",
        "* COMET-QE\n",
        "* TransQuest\n",
        "* DeepQuest"
      ],
      "metadata": {
        "id": "ah9hlkpZ5SOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation\n",
        "\n",
        "[Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation](https://aclanthology.org/2021.tacl-1.87) (Freitag et al., TACL 2021)\n",
        "\n",
        "[유튜브 보기](https://www.youtube.com/watch?reload=9&app=desktop&v=EBTaGnggVR0)\n",
        "\n",
        "### Abstract\n",
        "\n",
        "field still lacks a commonlyaccepted standard procedure\n",
        "\n",
        "toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework.\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Like many natural language generation tasks, machine translation (MT) is difficult to evaluate because the set of correct answers for each input is\n",
        "large and usually unknown.\n",
        "\n",
        "Yet even human evaluation is problematic\n",
        "\n",
        "there is a risk that the signal will become lost in rater noise or bias.\n",
        "\n",
        "This paper aims to contribute to the evolution of\n",
        "standard practices for human evaluation of highquality MT.\n",
        "\n",
        "Our key insight is that any scoring or ranking of translations is implicitly based on an identification of errors and other imperfections.\n",
        "\n",
        "MQM is a generic framework that provides a\n",
        "hierarchy of translation errors which can be tailored to specific applications. We identified a hierarchy appropriate for broad-coverage MT, and\n",
        "annotated outputs from 10 top-performing \"systems\" (including human references)\n",
        "\n",
        "Our main contributions are:\n",
        "\n",
        "* A proposal for a standard MQM scoring\n",
        "scheme appropriate for broad-coverage MT.\n",
        "* Demonstration that automatic metrics based on pre-trained embeddings can outperform human crowd workers.\n",
        "* Characterization of current error types in HT and MT, identifying specific MT weaknesses\n",
        "\n",
        "### MQM\n",
        "Table 12. MQM Guidelines\n",
        "\n",
        "Since we are ultimately interested in scoring\n",
        "segments, we require a weighting on error types.\n",
        "We fixed the weight on Minor errors at 1, and explored a range of Major weights from 1 to 10 (the\n",
        "Major weight recommended in the MQM standard). For each weight combination we examined\n",
        "the stability of system ranking using a resampling\n",
        "technique. We found that a Major weight of 5 gave\n",
        "the best balance between stability and ability to\n",
        "discriminate among systems.\n",
        "\n",
        "These weights apply to all error categories with\n",
        "two exceptions. We assigned a weight of 0.1 to\n",
        "Minor Fluency/Punctuation errors to reflect their\n",
        "mostly non-linguistic nature. Decisions like the\n",
        "style of quotation mark to use or the spacing\n",
        "around punctuation affect the appearance of a text\n",
        "but do not change its meaning. Unlike other kinds\n",
        "of Minor errors, these are easy to correct algorithmically, so we assign a low weight to ensure that\n",
        "their main role is to distinguish between systems\n",
        "that are equivalent in other respects. Major Fluency/Punctuation errors, which render a text ungrammatical or change its meaning (eg, eliding\n",
        "the comma in “Let’s eat, grandma”), have standard weighting. The second exception is the singleton Non-translation category, with a weight of\n",
        "25, equivalent to five Major errors.\n",
        "Table 1 summarizes our weighting scheme, in\n",
        "which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score\n",
        "is an average over scores from all annotators.\n",
        "\n",
        "| a | b | c |\n",
        "|---|---|---|\n",
        "| Major  | Non-translation  | 25  |\n",
        "| Major  | all others  |  5 |\n",
        "| Minor  | Fluency/Punctuation  | 0.1  |\n",
        "| Minor  | all others | 1  |\n",
        "| Netural  | all  | 0  |\n",
        "\n",
        "Table 1: MQM error weighting\n",
        "\n",
        "실험은 유튜브 참조\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "As part of this work, we proposed a standard\n",
        "MQM scoring scheme that is appropriate for high quality MT.\n",
        "\n",
        "Our study shows that crowd-worker human\n",
        "evaluations (as conducted by WMT) have low\n",
        "correlation with MQM, and the resulting system level rankings are quite different.\n",
        "\n",
        "MQM labels acquired with professional translators show a large\n",
        "gap between the quality of human and machine\n",
        "generated translations. This demonstrates that\n",
        "MT is still far from human parity. Furthermore,\n",
        "we characterize the current error types in human\n",
        "and machine translations, highlighting which error\n",
        "types are responsible for the difference between\n",
        "the two.\n",
        "\n",
        " We hope that researchers will use this\n",
        "as motivation to establish more error-type specific\n",
        "research directions.\n",
        "\n",
        "### 추가\n",
        "Many MQM schemes include an\n",
        "additional “Critical” severity which is worse than\n",
        "Major, but we dropped this because its definition\n",
        "is often context-specific, capturing errors that are\n",
        "disproportionately harmful for a particular application\n",
        "\n",
        "We felt that for broad coverage MT the\n",
        "distinction between Major and Critical was likely\n",
        "to be highly subjective, while Major errors (actual\n",
        "errors) would be easier to distinguish from Minor\n",
        "ones (imperfections). Neutral severity allows annotators to express subjective opinions about the\n",
        "translation without affecting its rating\n",
        "\n",
        "Annotator instructions are shown in Table 12.\n",
        "We kept these minimal because our raters were\n",
        "professionals with previous experience in assessing translation quality, including with MQM.\n",
        "There are many subtle issues that arise in error annotation, such as the correct way to translate units\n",
        "(eg, should 1 inch be translated as 1 Zoll, 1cm,\n",
        "or 2.54cm?), but we resisted the temptation to establish an extensive list of context-specific guidelines, relying instead on the judgment of our annotators. In order to temper the effect of long segments, we imposed a maximum of five errors per\n",
        "segment. For segments with more errors, we asked\n",
        "raters to identify only the five most severe. Thus\n",
        "we do not distinguish between segments containing five or more than five Major errors, although\n",
        "we do distinguish between segments with many\n",
        "identifiable errors and those that are categorized\n",
        "as entirely Non-translation. To focus our raters\n",
        "on careful error identification, and to provide potentially useful information for further studies, we\n",
        "had them highlight error spans in the text, following the conventions laid out in Table 12.\n",
        "\n",
        "Scoring\n",
        "Since we are ultimately interested in deriving\n",
        "scores for sentences, we require a weighting on\n",
        "error categories and severities. We set the weight\n",
        "on Minor errors to 1, and explored a range of Major error weights from 1 to 10 (the Major weight\n",
        "recommended in the MQM standard). For each\n",
        "weight combination we examined the stability of\n",
        "system ranking using a resampling technique. We\n",
        "found that a Major weight of 5 gave the best balance of stability and ability to discriminate among\n",
        "systems.\n",
        "These weights apply to all error categories except Fluency/Punctuation and Nontranslation. We assigned a weight of 0.1 for\n",
        "Fluency/Punctuation to reflect its mostly nonlinguistic character. Decisions like the kind of\n",
        "quotation mark to use or the spacing between\n",
        "words and punctuation affect the appearance of a\n",
        "text but do not change its meaning. Unlike other\n",
        "kinds of minor errors, these are easy to correct\n",
        "algorithmically, so we assign them a low weight\n",
        "to ensure that their main role is to distinguish\n",
        "between systems that are equivalent in other\n",
        "respects. Our decision is supported by evidence\n",
        "from professional translators, who tend to treat\n",
        "minor punctuation errors as insignificant for the\n",
        "purpose of scoring, even when they are required to\n",
        "annotate them within the MQM framework. Note\n",
        "that this category does not include punctuation\n",
        "errors that render a text ungrammatical or change\n",
        "its meaning (eg, eliding the comma in “Let’s eat,\n",
        "grandma”), which have the same weight as other\n",
        "Major errors. Source errors are ignored in our\n",
        "current study but give us the ability to discard\n",
        "badly garbled source sentences, which might be\n",
        "prevalent in certain genres. The singleton Nontranslation category has a weight of 25, equivalent\n",
        "to five Major errors, the worst segment-level score\n",
        "possible in our annotation scheme.\n",
        "Our current weighting ignores the text span of\n",
        "errors, as this provides little information relevant\n",
        "to scoring once severity and category are taken\n",
        "into account.\n",
        "Table 1 summarizes our weighting scheme. The\n",
        "score of a segment is the sum of all errors it contains, averaged over all annotators, and ranges\n",
        "from 0 (perfect) to 25 (maximally bad). Segment scores are averaged to provide document and system-level scores.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0B8QoV6pOfWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MQM Framework"
      ],
      "metadata": {
        "id": "0GY354J4QuvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expert-based Human Evaluations on TED and News Domain\n",
        "\n",
        "[Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expert-based Human Evaluations on TED and News Domain](https://aclanthology.org/2021.wmt-1.73) (Freitag et al., WMT 2021)\n",
        "\n",
        "기존 WMT의 경우 new data 기준 해당 저자는 metrics의 generalize\n",
        "and perform well across domains를 판단하기 위해 Ted talks 데이터를 추가\n",
        "\n",
        "### Ted Talks Test Suite\n",
        "\n",
        "A long standing question about automated MT evaluation metrics has been whether metrics generalize and perform well across domains\n",
        "\n",
        "The TED domain is quite different from the news domain, particularly in its more informal and disfluent language style, yet it covers a wide variety\n",
        "of topics and vocabularies.\n",
        "\n",
        "As we will see, the quality performance\n",
        "of these systems and their relative rankings can be\n",
        "quite different depending on the language pair, as\n",
        "these were not trained to yield the highest performance on the news or TED domain\n",
        "\n",
        "The quality of the underlying human ratings is critical and recent findings (Freitag et al., 2021) have shown that crowd-sourced human ratings are not reliable for high quality MT output.\n",
        "\n",
        "To temper the effect of long segments, we imposed a maximum of five errors per segment, instructing raters to choose the five most severe errors for segments containing more errors.\n",
        "\n",
        "->  구글이랑 같은거 썼다. weighting도 같음.\n",
        "\n",
        "Annotation for English→Russian was performed\n",
        "by Unbabel who used a single professional native\n",
        "language annotator with several years of translation\n",
        "error experience based on variations of the MQM\n",
        "framework (Lommel et al., 2014).\n",
        "\n",
        "The Unbabel\n",
        "severity options differ slightly from that of Google\n",
        "in that we also specify a ‘critical’ error severity and\n",
        "do not specify a ‘neutral’ category.\n",
        "\n",
        "-> 근데 구글이랑 좀 다르게 함.\n",
        "\n",
        "Additionally,\n",
        "in the Unbabel typology, all error categories are\n",
        "weighted equally within each severity level.\n",
        "\n",
        "MQM scores at a segment level are calculated\n",
        "by summing the number of errors in the segment\n",
        "in each severity and applying a severity weight as\n",
        "described in Table 3. In contrast to the Google\n",
        "scheme, Unbabel does not impose a limit on the\n",
        "number of errors in a segment.\n",
        "\n",
        "-> 계산 방식도 좀 다름 (Table 3 참고)\n",
        "\n",
        "### Google vs. Unbabel MQM\n",
        "\n",
        "Given that annotations were undertaken for\n",
        "English→Russian using a different setup and\n",
        "MQM scheme than those for English→German\n",
        "and English→Chinese we sought to provide some\n",
        "insight into the compatibility of the two schemes\n",
        "by repeating the annotation for English→German\n",
        "using Unbabel’s scheme and annotator pool\n",
        "\n",
        "We note that the Google annotators left 59.5% of\n",
        "the sample untouched (i.e. error free), whereas the\n",
        "Unbabel annotator left only 46.9% untouched. It\n",
        "appears that the Unbabel annotator was on average\n",
        "more aggressive in their annotation which might\n",
        "partially explain low levels of agreement.\n",
        "\n"
      ],
      "metadata": {
        "id": "tCKS28ok7RKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.statmt.org/wmt22/pdf/2022.wmt-1.3.pdf\n",
        "https://aclanthology.org/2022.wmt-1.57.pdf\n",
        "https://www.researchgate.net/figure/MQM-core-issue-types-used-for-the-word-level-annotation-task_fig1_273259476\n",
        "https://arxiv.org/pdf/2308.07286.pdf\n",
        "https://www.semanticscholar.org/paper/Word-Level-Quality-Estimation-for-Korean-English-Eo-Park/ec240fd52b5bd7338349a9d8bd147ab9aa341ba5\n",
        "https://www.taus.net/resources/blog/quality-estimation-for-machine-translation\n",
        "https://www.semanticscholar.org/paper/PATQUEST%3A-Papago-Translation-Quality-Estimation-Baek-Kim/a7f0a1baf1a962137629476156bb361136d00a3e\n",
        "https://choice-life.tistory.com/82\n",
        "https://www.w3.org/community/mqmcg/2018/10/04/draft-2018-10-04/\n",
        "https://sites.miis.edu/runyul/2018/03/04/translation-quality-assessment-mqm-multidimensional-quality-metrics/\n"
      ],
      "metadata": {
        "id": "hyQ0PSurt2gw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps3Iz4QysHY4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 과제 1"
      ],
      "metadata": {
        "id": "UZ7OvN-ickt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI\n",
        "\n",
        "https://arxiv.org/abs/2303.13336\n",
        "\n",
        "### Introduction\n",
        "\n",
        "the text-to-speech\n",
        "and speech enhancement task are two main active tasks, which **generates a speech from a given text and enhances**\n",
        "the quality of an existing speech, respectively.\n",
        "\n",
        "The development of text-to-speech task can be roughly divided into\n",
        "**three stages** [112]: **early works** (e.g., formant synthesis [40, 41, 95] and concatenative synthesis [29, 69, 75]), **statistical\n",
        "parametric speech synthesis (SPSS)-based methods** [115, 116, 133], and **neural network-based stage**\n",
        "\n",
        "Speech enhancement [74] is another active research field in\n",
        "speech area, which generates speech with a speech signal as input.\n",
        "\n",
        "this work conducts a survey on audio diffusion models\n",
        "\n",
        "While [111] reviews text-to-speech works, it mainly highlights the digital signal processing\n",
        "components. Other works [73, 74, 112] review speech synthesis based on deep learning, which give us an understanding\n",
        "of the development of neural text-to-speech or speech enhancement.\n",
        "\n",
        "### Background\n",
        "\n",
        "In essence, an audio waveform\n",
        "is a mixture of frequencies [14, 120], and therefore, audio analysis often starts with transforming the audio from its raw\n",
        "waveform in the time domain to the spectrogram in the time-frequency domain. This is achieved by segmenting the\n",
        "audio into windows, for which a short-time Fourier Transform (STFT) [64] calculate its magnitude for each frequency.\n",
        "\n",
        "The STFT is repeated for every window along the time direction, resulting in a map of complex values with two\n",
        "dimensions: X-axis representing frame (time) and Y-axis representing frequency.\n",
        "\n",
        "Since human are not sensitive to the frequency\n",
        "equally, a mel-scale is often adopted to transform the spectrogram (magnitude map) into Mel-spectrogram\n",
        "\n",
        "Mel은 달팽이관에서 모티브를 받았으며, 달팽이관은 특수한 성질이 있다.\n",
        "주파수가 낮은 대역에서는 주파수의 변화를 잘 감지하는데, 주파수가 높은 대역에서는 주파수의 변화를 잘 감지하지 못한다는 것이다.\n",
        "\n",
        "달팽이관의 구조로 살펴보면, 달팽이관에서 저주파 대역을 감지하는 부분은 굵지만\n",
        "고주파 대역을 감지하는 부분으로 갈수록 얇아진다\n",
        "그렇다면, 특징벡터로 그냥 주파수를 쓰기 보다는 이러한 달팽이관의 특성에 맞춰서 특징을 뽑아주는 것이  더욱 효과적인 피쳐를 뽑는 방법일 것이다.\n",
        "그래서 위와 같이 사람 달팽이관 특성을 고려한 값을 Mel-scale이라고 한다.\n",
        "\n",
        "### Overview of the text-to-speech development\n",
        "\n",
        "From three-stage to two-stage framework. The development of text-to-speech has undergone through a shift\n",
        "from a three-stage framework to a two-stage framework, as shown in Figure 1. Before applying neural networks\n",
        "\n",
        "statistical parametric speech synthesis (SPSS) was a popular method [115, 116, 132, 133, 137] consisting of three\n",
        "stages. As shown in Figure 1 (a), the text input is first converted to linguistic features, then acoustic features, and\n",
        "to the waveform in the last stage. Common acoustic features include mel-cepstral coefficients [18], Mel-generalized\n",
        "coefficients [114], F0 [34] and band aperiodicity [33]. Neural networks have brought a paradigm shift from three stages\n",
        "(Figure 1 (a)) to two stages (Figure 1 (b) and Figure 1 (c)). One branch of two-stage framework makes a deep vocoder\n",
        "directly generates waveform from linguistic features (Figure 1 (b)), such as WaveNet [118], Parallel Wavenet [76],\n",
        "DeepVoice 1 [2] DeepVoice 2 [20], HiFi-GAN [4]. **Currently, another two-stage framework is more dominant** as shown\n",
        "in Figure 1 (c), which directly generates Mel-Spectrogram form of acoustic features from text with a single deep acoustic\n",
        "model, including DeepVoice 3 [83], TransformerTTS [54], Fast Speech 1 [88] and Speech 2 [87].\n",
        "\n",
        "Overview of diffusion-based methods. Most recent text-to-speech works on diffusion model follow the two-stage\n",
        "framework in Figure 1 (c), which first generate acoustic features with a acoustic models, and then output waveform with\n",
        "a vocoder. Another branch of work attempts to solve the text-to-speech task in an end-to-end manner.\n",
        "\n",
        "### Acoustic model\n",
        "Acoustic model that transforms a text to acoustic features is a core component in the the task of text to speech. A\n",
        "summary of representative work that apply diffusion model to acoustic model is shown in Table 2.\n",
        "\n",
        "In speech synthesis systems, an acoustic model converts the text into acoustic features (e.g., Mel-spectrogram).\n",
        "Diff-TTS [31] is the first work that applies DDPM to el-spectrogram generation.\n",
        "\n",
        "Acceleration with knowledge distillation. According to ProDif\n",
        "\n",
        "Acceleration with Denoising Diffusion GANs. DiffGAN-TTS\n",
        "\n",
        "Adaptive modeling for multi-speaker setting. Grad-TTS\n",
        "\n",
        "Acoustic models with discrete latent space. Diffsound\n",
        "\n",
        "### Vocoder\n",
        "\n",
        "Neural vocoders generate waveform based on acoustic feature, e.g., Mel-spectrogram. In earlier researches on vocoders\n",
        "until 2020, autoregressive models have been popular in audio generation for their high-quality output samples but\n",
        "suffer from low inference speed. Although non-autoregressive methods improve the inference speed significantly by\n",
        "reducing sequential steps, there is still an audio quality gap between non-autoregressive and autoregressive methods.\n",
        "\n",
        "WaveGrad [7] is a pioneering work combing score matching and diffusion models by estimating the gradient of\n",
        "the data log-density, which bridges the audio quality gap between non-autoregressive and autoregressive methods.\n",
        "\n",
        "Another work\n",
        "DiffWave [45] is the first model showing a high versatility of waveform generation applications based on diffusion\n",
        "models. In the vocoder task, DiffWave [45] is conditioned on mel-spectrogram, and achieves comparable speech\n",
        "quality to the strong autoregressive methods. DiffWave [45] can also produce realistic voices and consistent word-level\n",
        "pronunciation in unconditional and class-conditional settings\n",
        "\n",
        "With a shared noise schedule for training and sampling, DDPM [24] requires thousands of sampling iterations\n",
        "for high-quality generation [48]. This property inspires investigations to speed up DDPM [24] by improving the\n",
        "noise schedule.\n",
        "\n",
        "Schedule prediction by additional networks. In the vocoder task,\n",
        "BDDM [48] can generates indistinguishable samples from human speech with only seven steps, 143x and 28.6x faster\n",
        "than WaveGrad [7] and DiffWave [45], respectively.\n",
        "\n",
        "Efficient inference by joint training. To reduce the inference iterations while maintaining the generation quality,\n",
        "InferGrad [9] proposes to incorporate the inference process into training with an additional loss.\n",
        "\n",
        "Improvement with noise prior. DDGM [70] improves the quality of generated\n",
        "audio than WaveGrad [7]. Quality improvement is also observed in the image generation area\n",
        "\n",
        "Another work [50] points\n",
        "out that the Gaussian noise prior may be insufficient to represent all modes of the samples (e.g., the different voiced andunvoiced segments), leading to a discrepancy between the real data distribution and the choice of prior and thus the\n",
        "training inefficiency. Thus, PriorGrad [50] proposes to apply an adaptive prior from the data statistics for the efficiency\n",
        "improvement of the conditional diffusion model for speech analysis.\n",
        "\n",
        "Other improvements. ItôWave [125] is the first to propose a vocoder based on linear Itô SDE. Based on Melspectrogram, ItôWave [125] achieves higher MOS with 95% confidence than WaveGrad [7] and DiffWave [45]. SpecGrad [44] proposes to adopt the spectral envelope of diffusion noise to the conditional log-mel spectrum, which improves\n",
        "the sound quality especially for the high-quality bands.\n",
        "\n",
        "### End-to-end frameworks\n",
        "\n",
        "Instead of treating acoustic modeling and vocoder modeling as independent processes, a branch of work evolves from\n",
        "partially end-to-end methods to fully end-to-end methods gradually.\n",
        "\n",
        "Resembling the two-stage frameworks, partially\n",
        "end-to-end methods [82, 107] also adopt two models as acoustic model and vocoder, but differentiates by training the\n",
        "two models in a joint manner\n",
        "\n",
        "By contrast, (fully) end-to-end frameworks adopt a single model to generate waveform\n",
        "from text without acoustic features as explicit representation.\n",
        "\n",
        "A line of fully end-to-end work adopts an adversarial\n",
        "decoder (or GAN), including FastSpeech 2 [87], EATS [15] and EFTS-Wav [65]. Most end-to-end methods still rely on\n",
        "generating mel-spectrogram for text-speech alignment, and a spectrogram-free flow-based method is investigated in\n",
        "Wave-Tacotron [123] by simply maximizing likelihood. A limitation of Wave-Tacotron [123] is that the decoder remains\n",
        "autoregressive making it at a disadvantage compared with non-autoregressive counterparts.\n",
        "\n",
        "Pioneering works. In contrast to WaveGrad [7] converting Mel-spectrogram to waveform, WaveGrad 2 [8] adopts an\n",
        "end-to-end manner that takes a phoneme sequence as input and generates the audio directly\n",
        "\n",
        "Generation of fullband audios. While previous work focusing on the generation of band-limited audios due\n",
        "to model constraints, DAG [79] adopts an end-to-end manner to generate full-band audios directly\n",
        "\n",
        "Model based on Itô SDE. Inspired by ItôWave [125], Itôn [99] proposes an end-to-end model for speech synthesis\n",
        "based on Itô SDE. Apart from the encoder-decoder architecture, Itôn [99] introduces a dual-denoiser structure for the generation of mel-spectrogram and waveform, respectively. Moreover, Itôn [99] adopts a two-stage training strategy\n",
        "that trains the encoder and Mel denoiser in the first stage, and the wave denoiser in the second stage.\n",
        "\n",
        "### SPEECH ENHANCEMENT\n",
        "\n",
        "Apart from text-to-speech generation, diffusion models have also been widely used in improving the quality of\n",
        "existing degraded audio.\n",
        "\n",
        "Numerous factors can cause the degradation of audio quality and we divide them into two\n",
        "classes according to the restoration type. The first branch of methods **removes perturbations** in the original clean audio,\n",
        "e.g., noise and reverb. The second branch **restores missing parts or adds the desired part**, e.g., audio super-resolution.\n",
        "\n",
        "Numerous deep speech enhancement methods have been investigated and they can be categorized into two classes. A\n",
        "discriminative method minimizes the difference between enhanced and clean speech [13, 16, 17, 42], while generative\n",
        "models are optimized by estimating the distribution of clean signals [51, 80, 86, 106, 108]. Despite a superior result\n",
        "regarding objective metrics, the discriminative class often suffers from sounding unnatural compared with the generative\n",
        "class. Diffusion model falling into the generative class is a promising method for bridging its gap with the discriminative\n",
        "class.\n",
        "\n",
        "### Enhancement by removing\n",
        "\n",
        "Two-stage refinement in speech enhancement. Research on speech enhancement has achieved significant\n",
        "improvement in terms of signal-to-noise (SNR) ratio but sometimes degrades the speech quality (e.g., naturalness),\n",
        "leading to the degradation of downstream applications. To remove the distortions of speech enhancement outputs,\n",
        "Refiner [93] applies a diffusion model pretrained on clean speech data to detect the degraded part, and then replaces them\n",
        "with newly generated clean ones in the manner of denoising diffusion restoration models(DDRM) [35]. Experimental\n",
        "results show that Refiner [93] is versatile since it improves speech quality with regard to various speech enhancement\n",
        "methods. Moreover, the Refiner [93] can also be integrated into the speech enhancement model for joint optimization\n",
        "in the future.\n",
        "\n",
        "### Enhancement by adding\n",
        "\n",
        "Audio super-resolution [46, 56], also widely known as upsampling [84] or bandwidth extension [46], aims to generate\n",
        "audio of a high sampling rate from that of a low sampling rate via extending its bandwidth\n",
        "\n",
        "Improved sampling method. Diffusion-based audio super-resolution is commonly conducted by conditioning the\n",
        "denoising network on low-resolution audio. [134] further improves the audio quality by injecting the low-resolution\n",
        "audio into the sampling process as a condition if the downsampling schedule is known\n",
        "\n",
        "Improved model architecture. [141] improves the speech quality caused by deterministic mathematical degradation, e.g., compression, clipping and downsampling.\n",
        "\n",
        "### Miscallenous audio tasks\n",
        "\n",
        "Voice conversion. Voice Conversion edits the source speakers to adapt to the target speaker by changing speech\n",
        "signal features. Traditional techniques include vector quantization [1, 100], hidden Markov models [36, 113], and\n",
        "Gausian mixture models [109, 121]. However, these methods only focus on specific parts of spectrum rather than the\n",
        "entire spectrum, resulting in poor speech quality. To solve these problems, [47, 66, 81, 110? ] adopt neural networks\n",
        "which cover the entire spectrum, and DiffSVC [59] is a pioneering work that leveraging diffusion models on voice\n",
        "conversion. Specifically, DiffSVC [59] is developed for singing voice conversion (SVC), which first transforms the\n",
        "phonetic posteriorgrams (PPGs) to spectral features, and then transforms them into waveforms by a trained neural\n",
        "vocoder. This method can get the target speech by any other speech as input (i.e., any-to-one SVC).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3KQcVf0cujN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TTS 정리\n",
        "\n",
        "https://music-audio-ai.tistory.com/44"
      ],
      "metadata": {
        "id": "Wh0NxXjZ9T_j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QZo0ZgynLOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}